{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd7ac25-9549-40fb-95e7-704828689c02",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb9240-adde-4fb5-8cd4-dd5c1db39b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy, matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Stable baselines\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "import os\n",
    "local_path = '/Users/riccardo/Documents/GitHub/' #'path_to_progect_folder/'\n",
    "sys.path.append(local_path+'OptimalControlAttacks/SyntheticDataExperiments/')\n",
    "from Modules import AuxiliaryFunctions as AF\n",
    "from Modules import GreedyAttacks as GA\n",
    "from Modules import DeepRLAttacks as RLA\n",
    "from Parameters import ParametersAttacksComparison_SigmoidalPerceptron as Par"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114b71d-1496-477a-8844-0699f5d46689",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559bc37-a2b8-4bcc-9f8b-bee3a6a7ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "orange = '#F5A962'\n",
    "light_blue = '#3C8DAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4618589-fcac-4a8e-9a86-5b6930378729",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa8b21-e7a8-4dce-9a29-7938df096043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation\n",
    "activation = Par.activation\n",
    "\n",
    "# Input data parameters\n",
    "dim_input = Par.dim_input\n",
    "batch_size = Par.batch_size\n",
    "mu_x = Par.mu_x\n",
    "sigma_x = Par.sigma_x\n",
    "n_runs_experiments = Par.n_runs_experiments\n",
    "\n",
    "# Dynamics parameters\n",
    "learning_rate = Par.learning_rate\n",
    "gamma = Par.gamma\n",
    "beta = Par.beta\n",
    "\n",
    "# N. samples\n",
    "n_timesteps = Par.n_timesteps\n",
    "n_timesteps_transient_th = Par.n_timesteps_transient_th\n",
    "n_timesteps_past = Par.n_timesteps_past\n",
    "n_samples_average = Par.n_samples_average\n",
    "n_samples_buffer = Par.n_samples_buffer\n",
    "n_samples_test = Par.n_samples_test\n",
    "time_window = Par.time_window\n",
    "\n",
    "# Control parameters\n",
    "a_min = Par.a_min\n",
    "a_max = Par.a_max\n",
    "n_a_gridpoints = Par.n_a_gridpoints\n",
    "n_runs_calibration = Par.n_runs_calibration\n",
    "control_cost_weight = Par.control_cost_weight\n",
    "greedy_weight_future = Par.greedy_weight_future\n",
    "opt_pref = Par.opt_pref\n",
    "fut_pref = Par.fut_pref\n",
    "\n",
    "# DeepRL Agent\n",
    "agent_model_name = Par.agent_model_name\n",
    "n_actions = Par.n_actions\n",
    "use_action_noise = Par.use_action_noise\n",
    "action_noise_mean = Par.action_noise_mean\n",
    "action_noise_std = Par.action_noise_std\n",
    "use_small_achitecture = Par.use_small_achitecture\n",
    "randomise_initial_condition = Par.randomise_initial_condition\n",
    "shuffle_array = Par.shuffle_array\n",
    "learning_rate_agent = Par.learning_rate_agent\n",
    "activation_fn = Par.activation_fn\n",
    "n_episodes = Par.n_episodes\n",
    "save_freq = Par.save_freq\n",
    "train_freq = Par.train_freq\n",
    "\n",
    "# Strings/paths\n",
    "experiment_description = Par.experiment_description\n",
    "path_agent = Par.path_agent\n",
    "export_path = Par.export_path\n",
    "rlmodels_path = Par.rlmodels_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cacbe8-0bec-4d2f-a891-2f2d7672e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dim_input:', dim_input)\n",
    "print('batch_size:', batch_size)\n",
    "print('activation:', activation)\n",
    "print('a min:', a_min)\n",
    "print('a max:', a_max)\n",
    "print('control_cost_weight:', control_cost_weight)\n",
    "print('N past timesteps:', n_timesteps_past)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c2160-13a5-4433-ba82-5c74f531ebbe",
   "metadata": {},
   "source": [
    "# Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdce99-5e89-4ca0-820a-f8341fd34beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher\n",
    "w_teach = np.ones(dim_input) #np.random.normal(0, 1, dim_input)\n",
    "w_teach = w_teach/(np.sum(w_teach**2)/dim_input)**0.5\n",
    "# Target\n",
    "w_target = -w_teach\n",
    "# Student (initial condition)\n",
    "w_stud_0 = w_teach\n",
    "            \n",
    "# Arrays (assuming batch_size as specified above)\n",
    "x_incoming_arr = np.random.normal(mu_x, sigma_x, (n_runs_experiments, batch_size*n_timesteps, dim_input))\n",
    "x_past = np.random.normal(mu_x, sigma_x, (batch_size*n_timesteps_past, dim_input))\n",
    "x_buffer = np.random.normal(mu_x, sigma_x, (batch_size*n_samples_buffer, dim_input))\n",
    "x_test = np.random.normal(mu_x, sigma_x, (batch_size*n_samples_test, dim_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a74ee2-f560-4fc6-8ac6-23a23e85c8a3",
   "metadata": {},
   "source": [
    "# Weight action cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bb0da-8267-402b-afd6-d1a73cbff836",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_t_test = np.dot(w_teach, x_test.T)/(dim_input**0.5)\n",
    "input_o_test = np.dot(w_target, x_test.T)/(dim_input**0.5)\n",
    "label_t_test = GA.perceptron(input_t_test, activation=activation)\n",
    "label_o_test = GA.perceptron(input_o_test, activation=activation)\n",
    "error_target_teach = np.mean((label_o_test-label_t_test)**2)\n",
    "d_target_teach = 0.5 * error_target_teach\n",
    "pref_control_cost_weight = (2*d_target_teach)\n",
    "print('Pre-factor control cost: %.2f' % pref_control_cost_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa3246-8abc-4ea4-9ba9-faedc9974514",
   "metadata": {},
   "source": [
    "## Check environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387dd4a-209a-47a4-8c76-7d2e760cf46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shuffle past stream:', shuffle_array)\n",
    "print('Randomise initial condition:', randomise_initial_condition)\n",
    "env = RLA.EnvironmentPerceptron(x_arr=x_past, \n",
    "                                batch_size=batch_size,\n",
    "                                w_stud_0=w_stud_0,\n",
    "                                w_teach=w_teach, \n",
    "                                w_target=w_target, \n",
    "                                a_min=a_min,\n",
    "                                a_max=a_max,\n",
    "                                learning_rate=learning_rate, \n",
    "                                control_cost_weight=pref_control_cost_weight*control_cost_weight, \n",
    "                                activation=activation,\n",
    "                                shuffle_array=shuffle_array, \n",
    "                                randomise_initial_condition=randomise_initial_condition)\n",
    "\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9244746b-a523-4ed5-9014-f200904aded2",
   "metadata": {},
   "source": [
    "# RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01092f40-37ae-4e4a-98a7-202f1b7339d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small architecture\n",
    "if use_small_achitecture:\n",
    "    print('Using custom small architecture for policy and value!')\n",
    "    custom_net_kwargs = dict(activation_fn=activation_fn, net_arch=dict(pi=[10, 10], qf=[10, 10]))\n",
    "else:\n",
    "    custom_net_kwargs = None\n",
    "         \n",
    "# Create model\n",
    "if agent_model_name=='TD3':\n",
    "    \n",
    "        if use_action_noise:\n",
    "            action_noise = NormalActionNoise(mean=action_noise_mean, sigma=action_noise_std)\n",
    "        else:\n",
    "            action_noise = None  \n",
    "            \n",
    "        model = TD3(\"MlpPolicy\", \n",
    "                    env,\n",
    "                    action_noise=action_noise,\n",
    "                    verbose=0, \n",
    "                    gamma=gamma,\n",
    "                    learning_rate=learning_rate_agent, \n",
    "                    policy_kwargs=custom_net_kwargs,\n",
    "                    train_freq=train_freq, \n",
    "                    gradient_steps=train_freq)\n",
    "        print('New %s agent created' % agent_model_name)\n",
    "else:\n",
    "    raise ValueError('Agent type other than TD3 not available/tested')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2dd4e-0be8-4b4c-84ed-0a11014ea226",
   "metadata": {},
   "source": [
    "# Training agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c65c75-1342-452c-9595-2973254964a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('timesteps for each episode:', n_timesteps_past)\n",
    "\n",
    "# Save a checkpoint \n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=save_freq,\n",
    "  save_path=path_agent,\n",
    "  name_prefix=\"rl_model\",\n",
    "  save_replay_buffer=False,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    print('Episode: %d/%d'%(ep+1, n_episodes))\n",
    "    model.learn(total_timesteps=n_timesteps_past, \n",
    "                progress_bar=False, \n",
    "                reset_num_timesteps=False,\n",
    "                tb_log_name='DeepRL', \n",
    "                log_interval=1, \n",
    "                callback=checkpoint_callback)\n",
    "        \n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf258824-0032-4df3-a232-5011374a5834",
   "metadata": {},
   "source": [
    "## Compute training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8b78d-878c-4243-b0ab-17df62b9e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = n_timesteps_past\n",
    "\n",
    "steps = []\n",
    "rew_disc_av = []\n",
    "\n",
    "# Test environment\n",
    "env = RLA.EnvironmentPerceptron(x_arr=x_test,\n",
    "                                batch_size=batch_size,\n",
    "                                w_stud_0=w_stud_0, \n",
    "                                w_teach=w_teach, \n",
    "                                w_target=w_target, \n",
    "                                a_min=a_min,\n",
    "                                a_max=a_max,\n",
    "                                learning_rate=learning_rate, \n",
    "                                control_cost_weight=pref_control_cost_weight*control_cost_weight, \n",
    "                                activation=activation, \n",
    "                                randomise_initial_condition=randomise_initial_condition, \n",
    "                                shuffle_array=shuffle_array)\n",
    "\n",
    "# Testing\n",
    "for ep in range(n_episodes):\n",
    "    \n",
    "    for t in range(0, timesteps+1, save_freq):\n",
    "        \n",
    "        timesteps_past = ep*timesteps + t\n",
    "        steps_flag = False\n",
    "\n",
    "        if agent_model_name=='TD3':\n",
    "            path_to_model = path_agent + f'/rl_model_{timesteps_past}_steps.zip'\n",
    "            #path_to_buffer = path_repbuffer + f'rl_model_replay_buffer_{timesteps_past}_steps.pkl'\n",
    "            if os.path.exists(path_to_model):\n",
    "                model_iter = TD3.load(path_to_model, env)\n",
    "                #model_iter.load_replay_buffer(path_to_buffer)\n",
    "                print('Model: %d steps (episode %d)' % (timesteps_past, ep+1)) \n",
    "                steps_flag = True\n",
    "            else:\n",
    "                print('Model not found: %d steps (episode %d)' % (timesteps_past, ep+1)) \n",
    "\n",
    "        if steps_flag:\n",
    "            reward_list = []\n",
    "            obs, info = env.reset()\n",
    "            terminated = False\n",
    "            while not terminated:\n",
    "                action, _ = model_iter.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                if env.timestep>=n_timesteps - time_window:\n",
    "                    reward_list.append(reward)\n",
    "                    \n",
    "            rew_disc_av.append(np.mean(np.array(reward_list, dtype=object)))\n",
    "            steps.append(timesteps_past)\n",
    "        \n",
    "objective = -np.array(rew_disc_av, dtype=object)\n",
    "steps = np.array(steps, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ef679-54b9-4f7f-b9ac-0ddea8a05ad6",
   "metadata": {},
   "source": [
    "## Export training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d79a43-5a17-4632-86b4-55cd60f9d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_description = '_agent#%s' % (agent_model_name)\n",
    "name = 'ObjectiveVSTrainingSteps'\n",
    "filename = name + '_@@@' + models_description + experiment_description\n",
    "data_to_export = objective\n",
    "np.save(rlmodels_path + filename, data_to_export)\n",
    "\n",
    "\n",
    "models_description = '_agent#%s' % (agent_model_name)\n",
    "name = 'TrainingSteps'\n",
    "filename = name + '_@@@' + models_description + experiment_description\n",
    "data_to_export = steps\n",
    "np.save(rlmodels_path + filename, data_to_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83423e-3a8d-4695-b612-361150096643",
   "metadata": {},
   "source": [
    "## Plot training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f53338c-f18f-4582-90f4-4bcbc37d24b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy average reward\n",
    "if activation=='Linear':\n",
    "    pref_fut = 1.\n",
    "elif activation=='Erf':\n",
    "    pref_fut = 2.\n",
    "reward_greedy_list = []\n",
    "\n",
    "# Greedy experiment\n",
    "obs, info = env.reset()\n",
    "w_stud = env.w_stud\n",
    "x_batch = env.x_batch\n",
    "terminated = False\n",
    "while not terminated:    \n",
    "    action = GA.a_greedy_perceptron(w_stud=w_stud,\n",
    "                                    w_target=w_target,\n",
    "                                    w_teach=w_teach,\n",
    "                                    x_batch=x_batch,\n",
    "                                    x_buffer=x_buffer,\n",
    "                                    dim_input=dim_input,\n",
    "                                    eta=learning_rate,\n",
    "                                    weight_future=pref_fut*dim_input/learning_rate,\n",
    "                                    a_min=a_min,\n",
    "                                    a_max=a_max,\n",
    "                                    control_cost_weight=pref_control_cost_weight*control_cost_weight,\n",
    "                                    activation=activation)\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    w_stud = env.w_stud\n",
    "    x_batch = env.x_batch\n",
    "    if env.timestep>=n_timesteps - time_window:\n",
    "        reward_greedy_list.append(reward)\n",
    "    \n",
    "objective_greedy = -np.mean(np.array(reward_greedy_list, dtype=object))\n",
    "objective_greedy, env.timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f6fe6-6357-4b35-aab3-b5a685ea9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_model_t = steps[np.argmin(objective)]\n",
    "best_model_obj = objective[np.argmin(objective)]\n",
    "best_model_t, best_model_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b25be-a4b0-4c7d-9d33-3b2e7472809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AF.SetPlotParams(magnification=.9, ratio=float(2.2/3.), fontsize=11, lines_w=.7, ms=7)\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rc('text', usetex = True)\n",
    "mpl.rc('text.latex', preamble=r'\\usepackage{sfmath}')\n",
    "\n",
    "\n",
    "plt.axhline(objective_greedy, color='black', ls=':', lw=.8, label='greedy')\n",
    "plt.plot(steps+1, objective, color=orange)\n",
    "plt.plot(best_model_t, best_model_obj, color=light_blue, ls='', marker='.', ms=8)\n",
    "\n",
    "plt.ylabel('$g^{\\mathrm{RL}}_{\\infty}$')\n",
    "plt.xlabel('n. episodes')\n",
    "plt.xlim([-n_timesteps_past/4, n_episodes*n_timesteps_past])\n",
    "x_positions_ticks = np.linspace(0, n_episodes, n_episodes//2+1)*n_timesteps_past\n",
    "x_ticks_labels = [int(x) for x in np.linspace(0, n_episodes, n_episodes//2+1)]\n",
    "plt.xticks(x_positions_ticks, x_ticks_labels)\n",
    "plt.yticks([0.2, 0.4, 0.6, 0.8])\n",
    "plt.ylim([0.2, .8])\n",
    "plt.grid(False)\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aef6dc-04ce-4da1-a9bd-94fa2a0fa528",
   "metadata": {},
   "source": [
    "# Plot 1D policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3a901-80d0-49dd-8c73-8c378b0d2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dim_input==1 and batch_size==1:\n",
    "    \n",
    "    ########################\n",
    "    #     Choose model     #\n",
    "    ########################\n",
    "    \n",
    "    timesteps_past = best_model_t\n",
    "    if agent_model_name=='TD3':\n",
    "        path_to_model = path_agent + f'/rl_model_{timesteps_past}_steps.zip'\n",
    "        #path_to_buffer = path_repbuffer + f'rl_model_replay_buffer_{timesteps_past}_steps.pkl'\n",
    "        if os.path.exists(path_to_model):\n",
    "            model_iter = TD3.load(path_to_model, env)\n",
    "            #model_iter.load_replay_buffer(path_to_buffer)\n",
    "            print('%d (ep %d)' % (timesteps_past, ep+1)) \n",
    "            steps_flag = True\n",
    "        else:\n",
    "            print('  %d-model not found (ep %d)' % (timesteps_past, ep+1)) \n",
    "\n",
    "    AF.SetPlotParams(magnification=1.6, ratio=float(2.2/3.), fontsize=12, lines_w=1.3, ms=7)\n",
    "    mpl.rc('text', usetex = True)\n",
    "    mpl.rc('text.latex', preamble=r'\\usepackage{sfmath}')\n",
    "\n",
    "    \n",
    "    ########################\n",
    "    #        3D Plot       #\n",
    "    ########################\n",
    "    \n",
    "    #******  Evaluate policy functions  ******#\n",
    "    n_rho_values = 60\n",
    "    rho_grid = np.linspace(0., 1., n_rho_values)\n",
    "    n_x_values = 60\n",
    "    x_grid = np.linspace(-3, 3, n_x_values)\n",
    "    a_greedy_grid_2D = np.zeros((n_rho_values, n_x_values))\n",
    "    a_DeepRL_grid_2D = np.zeros((n_rho_values, n_x_values))\n",
    "    for j, rho in enumerate(rho_grid):\n",
    "        print('Line: %d/%d' % (j+1, n_rho_values))\n",
    "        w_stud_val = w_teach * (1-rho) + w_target * rho\n",
    "        a_greedy_grid = np.zeros_like(x_grid)\n",
    "        a_DeepRL_grid = np.zeros_like(x_grid)\n",
    "        for i, x in enumerate(x_grid):\n",
    "            x_batch = np.array([[x]], dtype=np.float32)\n",
    "            obs = np.concatenate((w_stud_val.reshape(1,-1), x_batch), axis=0, dtype=np.float32).flatten()\n",
    "            a_DeepRL_grid[i], _ = model_iter.predict(obs, deterministic=True)\n",
    "            a_greedy_grid[i] = GA.a_greedy_perceptron(w_stud=w_stud_val,\n",
    "                                                      w_target=w_target,\n",
    "                                                      w_teach=w_teach,\n",
    "                                                      x_batch=x_batch,\n",
    "                                                      x_buffer=x_buffer,\n",
    "                                                      dim_input=dim_input,\n",
    "                                                      eta=learning_rate,\n",
    "                                                      weight_future=pref_fut*dim_input/learning_rate,\n",
    "                                                      a_min=a_min,\n",
    "                                                      a_max=a_max,\n",
    "                                                      control_cost_weight=pref_control_cost_weight*control_cost_weight,\n",
    "                                                      activation=activation)\n",
    "        a_greedy_grid_2D[j] = a_greedy_grid\n",
    "        a_DeepRL_grid_2D[j] = a_DeepRL_grid\n",
    "        \n",
    "    \n",
    "    #******  Greedy policy  ******#\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    alpha = 0.8\n",
    "    \n",
    "    x = x_grid\n",
    "    for j, rho in enumerate(rho_grid):\n",
    "        y = 1-rho * np.ones_like(x)\n",
    "        z = a_greedy_grid_2D[j]\n",
    "        ax.plot(x, y, z, color=light_blue, alpha=alpha)\n",
    "        ax.set(xlabel='x', ylabel='$\\\\rho$', zlabel='a')\n",
    "        ax.set_zlabel('a')\n",
    "\n",
    "    # Remove fill\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    # Set color of edges\n",
    "    ax.xaxis.pane.set_edgecolor('grey')\n",
    "    ax.yaxis.pane.set_edgecolor('grey')\n",
    "    ax.zaxis.pane.set_edgecolor('grey')\n",
    "    ax.grid(False)\n",
    "    # Save figure\n",
    "    #filename = 'Policy1DGreedy#%s.pdf' % activation\n",
    "    #plt.savefig(path_figures + filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #******  RL agent policy  ******#\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    \n",
    "    for j, rho in enumerate(rho_grid):\n",
    "        y = 1-rho * np.ones_like(x)\n",
    "        z = a_DeepRL_grid_2D[j]\n",
    "        ax.plot(x, y, z, color=orange, alpha=alpha)\n",
    "        ax.set(xlabel='x', ylabel='$\\\\rho$', zlabel='a')\n",
    "        ax.set_zlabel('a')\n",
    "\n",
    "    # Remove fill\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    # Set color of edges\n",
    "    ax.xaxis.pane.set_edgecolor('grey')\n",
    "    ax.yaxis.pane.set_edgecolor('grey')\n",
    "    ax.zaxis.pane.set_edgecolor('grey')\n",
    "    ax.grid(False)\n",
    "    # Save figure\n",
    "    #filename = 'Policy1DRL#%s.pdf' % activation\n",
    "    #plt.savefig(path_figures + filename, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OptimalControlAttaks",
   "language": "python",
   "name": "optimalcontrolattaks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
