{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd7ac25-9549-40fb-95e7-704828689c02",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08eb9240-adde-4fb5-8cd4-dd5c1db39b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy, matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Stable baselines\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "import os\n",
    "local_path = '/Users/riccardo/Documents/GitHub/' #'path_to_progect_folder/'\n",
    "sys.path.append(local_path+'OptimalControlAttacks/SyntheticDataExperiments/')\n",
    "from Modules import AuxiliaryFunctions as AF\n",
    "from Modules import GreedyAttacks as GA\n",
    "from Modules import DeepRLAttacks as RLA\n",
    "from Parameters import ParametersAttacksComparison_SigmoidalPerceptron as Par"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114b71d-1496-477a-8844-0699f5d46689",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f559bc37-a2b8-4bcc-9f8b-bee3a6a7ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "orange = '#F5A962'\n",
    "light_blue = '#3C8DAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4618589-fcac-4a8e-9a86-5b6930378729",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffa8b21-e7a8-4dce-9a29-7938df096043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation\n",
    "activation = Par.activation\n",
    "\n",
    "# Input data parameters\n",
    "dim_input = Par.dim_input\n",
    "batch_size = Par.batch_size\n",
    "mu_x = Par.mu_x\n",
    "sigma_x = Par.sigma_x\n",
    "n_runs_experiments = Par.n_runs_experiments\n",
    "\n",
    "# Dynamics parameters\n",
    "learning_rate = Par.learning_rate\n",
    "gamma = Par.gamma\n",
    "beta = Par.beta\n",
    "\n",
    "# N. samples\n",
    "n_timesteps = Par.n_timesteps\n",
    "n_timesteps_transient_th = Par.n_timesteps_transient_th\n",
    "n_timesteps_past = Par.n_timesteps_past\n",
    "n_samples_average = Par.n_samples_average\n",
    "n_samples_buffer = Par.n_samples_buffer\n",
    "n_samples_test = Par.n_samples_test\n",
    "time_window = Par.time_window\n",
    "\n",
    "# Control parameters\n",
    "a_min = Par.a_min\n",
    "a_max = Par.a_max\n",
    "n_a_gridpoints = Par.n_a_gridpoints\n",
    "n_runs_calibration = Par.n_runs_calibration\n",
    "control_cost_weight = Par.control_cost_weight\n",
    "greedy_weight_future = Par.greedy_weight_future\n",
    "opt_pref = Par.opt_pref\n",
    "fut_pref = Par.fut_pref\n",
    "\n",
    "# DeepRL Agent\n",
    "agent_model_name = Par.agent_model_name\n",
    "n_actions = Par.n_actions\n",
    "use_action_noise = Par.use_action_noise\n",
    "action_noise_mean = Par.action_noise_mean\n",
    "action_noise_std = Par.action_noise_std\n",
    "use_small_achitecture = Par.use_small_achitecture\n",
    "randomise_initial_condition = Par.randomise_initial_condition\n",
    "shuffle_array = Par.shuffle_array\n",
    "learning_rate_agent = Par.learning_rate_agent\n",
    "activation_fn = Par.activation_fn\n",
    "n_episodes = Par.n_episodes\n",
    "save_freq = Par.save_freq\n",
    "train_freq = Par.train_freq\n",
    "\n",
    "# Strings/paths\n",
    "experiment_description = Par.experiment_description\n",
    "path_agent = Par.path_agent\n",
    "export_path = Par.export_path\n",
    "rlmodels_path = Par.rlmodels_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cacbe8-0bec-4d2f-a891-2f2d7672e67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_input: 10\n",
      "batch_size: 1\n",
      "activation: Erf\n",
      "a min: -2\n",
      "a max: 3\n",
      "control_cost_weight: 1.0\n",
      "N past timesteps: 40000\n"
     ]
    }
   ],
   "source": [
    "print('dim_input:', dim_input)\n",
    "print('batch_size:', batch_size)\n",
    "print('activation:', activation)\n",
    "print('a min:', a_min)\n",
    "print('a max:', a_max)\n",
    "print('control_cost_weight:', control_cost_weight)\n",
    "print('N past timesteps:', n_timesteps_past)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c2160-13a5-4433-ba82-5c74f531ebbe",
   "metadata": {},
   "source": [
    "# Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25fdce99-5e89-4ca0-820a-f8341fd34beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher\n",
    "w_teach = np.ones(dim_input) #np.random.normal(0, 1, dim_input)\n",
    "w_teach = w_teach/(np.sum(w_teach**2)/dim_input)**0.5\n",
    "# Target\n",
    "w_target = -w_teach\n",
    "# Student (initial condition)\n",
    "w_stud_0 = w_teach\n",
    "            \n",
    "# Arrays (assuming batch_size as specified above)\n",
    "x_incoming_arr = np.random.normal(mu_x, sigma_x, (n_runs_experiments, batch_size*n_timesteps, dim_input))\n",
    "x_past = np.random.normal(mu_x, sigma_x, (batch_size*n_timesteps_past, dim_input))\n",
    "x_buffer = np.random.normal(mu_x, sigma_x, (batch_size*n_samples_buffer, dim_input))\n",
    "x_test = np.random.normal(mu_x, sigma_x, (batch_size*n_samples_test, dim_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a74ee2-f560-4fc6-8ac6-23a23e85c8a3",
   "metadata": {},
   "source": [
    "# Weight action cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "788bb0da-8267-402b-afd6-d1a73cbff836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-factor control cost: 1.32\n"
     ]
    }
   ],
   "source": [
    "input_t_test = np.dot(w_teach, x_test.T)/(dim_input**0.5)\n",
    "input_o_test = np.dot(w_target, x_test.T)/(dim_input**0.5)\n",
    "label_t_test = GA.perceptron(input_t_test, activation=activation)\n",
    "label_o_test = GA.perceptron(input_o_test, activation=activation)\n",
    "error_target_teach = np.mean((label_o_test-label_t_test)**2)\n",
    "d_target_teach = 0.5 * error_target_teach\n",
    "pref_control_cost_weight = (2*d_target_teach)\n",
    "print('Pre-factor control cost: %.2f' % pref_control_cost_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa3246-8abc-4ea4-9ba9-faedc9974514",
   "metadata": {},
   "source": [
    "## Check environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3387dd4a-209a-47a4-8c76-7d2e760cf46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle past stream: True\n",
      "Randomise initial condition: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/OptimalControlAttacks/lib/python3.8/site-packages/stable_baselines3/common/env_checker.py:428: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Shuffle past stream:', shuffle_array)\n",
    "print('Randomise initial condition:', randomise_initial_condition)\n",
    "env = RLA.EnvironmentPerceptron(x_arr=x_past, \n",
    "                                batch_size=batch_size,\n",
    "                                w_stud_0=w_stud_0,\n",
    "                                w_teach=w_teach, \n",
    "                                w_target=w_target, \n",
    "                                a_min=a_min,\n",
    "                                a_max=a_max,\n",
    "                                learning_rate=learning_rate, \n",
    "                                control_cost_weight=pref_control_cost_weight*control_cost_weight, \n",
    "                                activation=activation,\n",
    "                                shuffle_array=shuffle_array, \n",
    "                                randomise_initial_condition=randomise_initial_condition)\n",
    "\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9244746b-a523-4ed5-9014-f200904aded2",
   "metadata": {},
   "source": [
    "# RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01092f40-37ae-4e4a-98a7-202f1b7339d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New TD3 agent created\n"
     ]
    }
   ],
   "source": [
    "# Small architecture\n",
    "if use_small_achitecture:\n",
    "    print('Using custom small architecture for policy and value!')\n",
    "    custom_net_kwargs = dict(activation_fn=activation_fn, net_arch=dict(pi=[10, 10], qf=[10, 10]))\n",
    "else:\n",
    "    custom_net_kwargs = None\n",
    "         \n",
    "# Create model\n",
    "if agent_model_name=='TD3':\n",
    "    \n",
    "        if use_action_noise:\n",
    "            action_noise = NormalActionNoise(mean=action_noise_mean, sigma=action_noise_std)\n",
    "        else:\n",
    "            action_noise = None  \n",
    "            \n",
    "        model = TD3(\"MlpPolicy\", \n",
    "                    env,\n",
    "                    action_noise=action_noise,\n",
    "                    verbose=0, \n",
    "                    gamma=gamma,\n",
    "                    learning_rate=learning_rate_agent, \n",
    "                    policy_kwargs=custom_net_kwargs,\n",
    "                    train_freq=train_freq, \n",
    "                    gradient_steps=train_freq)\n",
    "        print('New %s agent created' % agent_model_name)\n",
    "else:\n",
    "    raise ValueError('Agent type other than TD3 not available/tested')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2dd4e-0be8-4b4c-84ed-0a11014ea226",
   "metadata": {},
   "source": [
    "# Training agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9c65c75-1342-452c-9595-2973254964a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps for each episode: 40000\n",
      "Episode: 1/8\n",
      "Episode: 2/8\n",
      "Episode: 3/8\n",
      "Episode: 4/8\n",
      "Episode: 5/8\n",
      "Episode: 6/8\n",
      "Episode: 7/8\n",
      "Episode: 8/8\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "print('timesteps for each episode:', n_timesteps_past)\n",
    "\n",
    "# Save a checkpoint \n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=save_freq,\n",
    "  save_path=path_agent,\n",
    "  name_prefix=\"rl_model\",\n",
    "  save_replay_buffer=False,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    print('Episode: %d/%d'%(ep+1, n_episodes))\n",
    "    model.learn(total_timesteps=n_timesteps_past, \n",
    "                progress_bar=False, \n",
    "                reset_num_timesteps=False,\n",
    "                tb_log_name='DeepRL', \n",
    "                log_interval=1, \n",
    "                callback=checkpoint_callback)\n",
    "        \n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf258824-0032-4df3-a232-5011374a5834",
   "metadata": {},
   "source": [
    "## Compute training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90a8b78d-878c-4243-b0ab-17df62b9e620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found: 0 steps (episode 1)\n",
      "Model: 1000 steps (episode 1)\n",
      "Model: 2000 steps (episode 1)\n",
      "Model: 3000 steps (episode 1)\n",
      "Model: 4000 steps (episode 1)\n",
      "Model: 5000 steps (episode 1)\n",
      "Model: 6000 steps (episode 1)\n",
      "Model: 7000 steps (episode 1)\n",
      "Model: 8000 steps (episode 1)\n",
      "Model: 9000 steps (episode 1)\n",
      "Model: 10000 steps (episode 1)\n",
      "Model: 11000 steps (episode 1)\n",
      "Model: 12000 steps (episode 1)\n",
      "Model: 13000 steps (episode 1)\n",
      "Model: 14000 steps (episode 1)\n",
      "Model: 15000 steps (episode 1)\n",
      "Model: 16000 steps (episode 1)\n",
      "Model: 17000 steps (episode 1)\n",
      "Model: 18000 steps (episode 1)\n",
      "Model: 19000 steps (episode 1)\n",
      "Model: 20000 steps (episode 1)\n",
      "Model: 21000 steps (episode 1)\n",
      "Model: 22000 steps (episode 1)\n",
      "Model: 23000 steps (episode 1)\n",
      "Model: 24000 steps (episode 1)\n",
      "Model: 25000 steps (episode 1)\n",
      "Model: 26000 steps (episode 1)\n",
      "Model: 27000 steps (episode 1)\n",
      "Model: 28000 steps (episode 1)\n",
      "Model: 29000 steps (episode 1)\n",
      "Model: 30000 steps (episode 1)\n",
      "Model: 31000 steps (episode 1)\n",
      "Model: 32000 steps (episode 1)\n",
      "Model: 33000 steps (episode 1)\n",
      "Model: 34000 steps (episode 1)\n",
      "Model: 35000 steps (episode 1)\n",
      "Model: 36000 steps (episode 1)\n",
      "Model: 37000 steps (episode 1)\n",
      "Model: 38000 steps (episode 1)\n",
      "Model: 39000 steps (episode 1)\n",
      "Model: 40000 steps (episode 1)\n",
      "Model: 40000 steps (episode 2)\n",
      "Model: 41000 steps (episode 2)\n",
      "Model: 42000 steps (episode 2)\n",
      "Model: 43000 steps (episode 2)\n",
      "Model: 44000 steps (episode 2)\n",
      "Model: 45000 steps (episode 2)\n",
      "Model: 46000 steps (episode 2)\n",
      "Model: 47000 steps (episode 2)\n",
      "Model: 48000 steps (episode 2)\n",
      "Model: 49000 steps (episode 2)\n",
      "Model: 50000 steps (episode 2)\n",
      "Model: 51000 steps (episode 2)\n",
      "Model: 52000 steps (episode 2)\n",
      "Model: 53000 steps (episode 2)\n",
      "Model: 54000 steps (episode 2)\n",
      "Model: 55000 steps (episode 2)\n",
      "Model: 56000 steps (episode 2)\n",
      "Model: 57000 steps (episode 2)\n",
      "Model: 58000 steps (episode 2)\n",
      "Model: 59000 steps (episode 2)\n",
      "Model: 60000 steps (episode 2)\n",
      "Model: 61000 steps (episode 2)\n",
      "Model: 62000 steps (episode 2)\n",
      "Model: 63000 steps (episode 2)\n",
      "Model: 64000 steps (episode 2)\n",
      "Model: 65000 steps (episode 2)\n",
      "Model: 66000 steps (episode 2)\n",
      "Model: 67000 steps (episode 2)\n",
      "Model: 68000 steps (episode 2)\n",
      "Model: 69000 steps (episode 2)\n",
      "Model: 70000 steps (episode 2)\n",
      "Model: 71000 steps (episode 2)\n",
      "Model: 72000 steps (episode 2)\n",
      "Model: 73000 steps (episode 2)\n",
      "Model: 74000 steps (episode 2)\n",
      "Model: 75000 steps (episode 2)\n",
      "Model: 76000 steps (episode 2)\n",
      "Model: 77000 steps (episode 2)\n",
      "Model: 78000 steps (episode 2)\n",
      "Model: 79000 steps (episode 2)\n",
      "Model: 80000 steps (episode 2)\n",
      "Model: 80000 steps (episode 3)\n",
      "Model: 81000 steps (episode 3)\n",
      "Model: 82000 steps (episode 3)\n",
      "Model: 83000 steps (episode 3)\n",
      "Model: 84000 steps (episode 3)\n",
      "Model: 85000 steps (episode 3)\n",
      "Model: 86000 steps (episode 3)\n",
      "Model: 87000 steps (episode 3)\n",
      "Model: 88000 steps (episode 3)\n",
      "Model: 89000 steps (episode 3)\n",
      "Model: 90000 steps (episode 3)\n",
      "Model: 91000 steps (episode 3)\n",
      "Model: 92000 steps (episode 3)\n",
      "Model: 93000 steps (episode 3)\n",
      "Model: 94000 steps (episode 3)\n",
      "Model: 95000 steps (episode 3)\n",
      "Model: 96000 steps (episode 3)\n",
      "Model: 97000 steps (episode 3)\n",
      "Model: 98000 steps (episode 3)\n",
      "Model: 99000 steps (episode 3)\n",
      "Model: 100000 steps (episode 3)\n",
      "Model: 101000 steps (episode 3)\n",
      "Model: 102000 steps (episode 3)\n",
      "Model: 103000 steps (episode 3)\n",
      "Model: 104000 steps (episode 3)\n",
      "Model: 105000 steps (episode 3)\n",
      "Model: 106000 steps (episode 3)\n",
      "Model: 107000 steps (episode 3)\n",
      "Model: 108000 steps (episode 3)\n",
      "Model: 109000 steps (episode 3)\n",
      "Model: 110000 steps (episode 3)\n",
      "Model: 111000 steps (episode 3)\n",
      "Model: 112000 steps (episode 3)\n",
      "Model: 113000 steps (episode 3)\n",
      "Model: 114000 steps (episode 3)\n",
      "Model: 115000 steps (episode 3)\n",
      "Model: 116000 steps (episode 3)\n",
      "Model: 117000 steps (episode 3)\n",
      "Model: 118000 steps (episode 3)\n",
      "Model: 119000 steps (episode 3)\n",
      "Model: 120000 steps (episode 3)\n",
      "Model: 120000 steps (episode 4)\n",
      "Model: 121000 steps (episode 4)\n",
      "Model: 122000 steps (episode 4)\n",
      "Model: 123000 steps (episode 4)\n",
      "Model: 124000 steps (episode 4)\n",
      "Model: 125000 steps (episode 4)\n",
      "Model: 126000 steps (episode 4)\n",
      "Model: 127000 steps (episode 4)\n",
      "Model: 128000 steps (episode 4)\n",
      "Model: 129000 steps (episode 4)\n",
      "Model: 130000 steps (episode 4)\n",
      "Model: 131000 steps (episode 4)\n",
      "Model: 132000 steps (episode 4)\n",
      "Model: 133000 steps (episode 4)\n",
      "Model: 134000 steps (episode 4)\n",
      "Model: 135000 steps (episode 4)\n",
      "Model: 136000 steps (episode 4)\n",
      "Model: 137000 steps (episode 4)\n",
      "Model: 138000 steps (episode 4)\n",
      "Model: 139000 steps (episode 4)\n",
      "Model: 140000 steps (episode 4)\n",
      "Model: 141000 steps (episode 4)\n",
      "Model: 142000 steps (episode 4)\n",
      "Model: 143000 steps (episode 4)\n",
      "Model: 144000 steps (episode 4)\n",
      "Model: 145000 steps (episode 4)\n",
      "Model: 146000 steps (episode 4)\n",
      "Model: 147000 steps (episode 4)\n",
      "Model: 148000 steps (episode 4)\n",
      "Model: 149000 steps (episode 4)\n",
      "Model: 150000 steps (episode 4)\n",
      "Model: 151000 steps (episode 4)\n",
      "Model: 152000 steps (episode 4)\n",
      "Model: 153000 steps (episode 4)\n",
      "Model: 154000 steps (episode 4)\n",
      "Model: 155000 steps (episode 4)\n",
      "Model: 156000 steps (episode 4)\n",
      "Model: 157000 steps (episode 4)\n",
      "Model: 158000 steps (episode 4)\n",
      "Model: 159000 steps (episode 4)\n",
      "Model: 160000 steps (episode 4)\n",
      "Model: 160000 steps (episode 5)\n",
      "Model: 161000 steps (episode 5)\n",
      "Model: 162000 steps (episode 5)\n",
      "Model: 163000 steps (episode 5)\n",
      "Model: 164000 steps (episode 5)\n",
      "Model: 165000 steps (episode 5)\n",
      "Model: 166000 steps (episode 5)\n",
      "Model: 167000 steps (episode 5)\n",
      "Model: 168000 steps (episode 5)\n",
      "Model: 169000 steps (episode 5)\n",
      "Model: 170000 steps (episode 5)\n",
      "Model: 171000 steps (episode 5)\n",
      "Model: 172000 steps (episode 5)\n",
      "Model: 173000 steps (episode 5)\n",
      "Model: 174000 steps (episode 5)\n",
      "Model: 175000 steps (episode 5)\n",
      "Model: 176000 steps (episode 5)\n",
      "Model: 177000 steps (episode 5)\n",
      "Model: 178000 steps (episode 5)\n",
      "Model: 179000 steps (episode 5)\n",
      "Model: 180000 steps (episode 5)\n",
      "Model: 181000 steps (episode 5)\n",
      "Model: 182000 steps (episode 5)\n",
      "Model: 183000 steps (episode 5)\n",
      "Model: 184000 steps (episode 5)\n",
      "Model: 185000 steps (episode 5)\n",
      "Model: 186000 steps (episode 5)\n",
      "Model: 187000 steps (episode 5)\n",
      "Model: 188000 steps (episode 5)\n",
      "Model: 189000 steps (episode 5)\n",
      "Model: 190000 steps (episode 5)\n",
      "Model: 191000 steps (episode 5)\n",
      "Model: 192000 steps (episode 5)\n",
      "Model: 193000 steps (episode 5)\n",
      "Model: 194000 steps (episode 5)\n",
      "Model: 195000 steps (episode 5)\n",
      "Model: 196000 steps (episode 5)\n",
      "Model: 197000 steps (episode 5)\n",
      "Model: 198000 steps (episode 5)\n",
      "Model: 199000 steps (episode 5)\n",
      "Model: 200000 steps (episode 5)\n",
      "Model: 200000 steps (episode 6)\n",
      "Model: 201000 steps (episode 6)\n",
      "Model: 202000 steps (episode 6)\n",
      "Model: 203000 steps (episode 6)\n",
      "Model: 204000 steps (episode 6)\n",
      "Model: 205000 steps (episode 6)\n",
      "Model: 206000 steps (episode 6)\n",
      "Model: 207000 steps (episode 6)\n",
      "Model: 208000 steps (episode 6)\n",
      "Model: 209000 steps (episode 6)\n",
      "Model: 210000 steps (episode 6)\n",
      "Model: 211000 steps (episode 6)\n",
      "Model: 212000 steps (episode 6)\n",
      "Model: 213000 steps (episode 6)\n",
      "Model: 214000 steps (episode 6)\n",
      "Model: 215000 steps (episode 6)\n",
      "Model: 216000 steps (episode 6)\n",
      "Model: 217000 steps (episode 6)\n",
      "Model: 218000 steps (episode 6)\n",
      "Model: 219000 steps (episode 6)\n",
      "Model: 220000 steps (episode 6)\n",
      "Model: 221000 steps (episode 6)\n",
      "Model: 222000 steps (episode 6)\n",
      "Model: 223000 steps (episode 6)\n",
      "Model: 224000 steps (episode 6)\n",
      "Model: 225000 steps (episode 6)\n",
      "Model: 226000 steps (episode 6)\n",
      "Model: 227000 steps (episode 6)\n",
      "Model: 228000 steps (episode 6)\n",
      "Model: 229000 steps (episode 6)\n",
      "Model: 230000 steps (episode 6)\n",
      "Model: 231000 steps (episode 6)\n",
      "Model: 232000 steps (episode 6)\n",
      "Model: 233000 steps (episode 6)\n",
      "Model: 234000 steps (episode 6)\n",
      "Model: 235000 steps (episode 6)\n",
      "Model: 236000 steps (episode 6)\n",
      "Model: 237000 steps (episode 6)\n",
      "Model: 238000 steps (episode 6)\n",
      "Model: 239000 steps (episode 6)\n",
      "Model: 240000 steps (episode 6)\n",
      "Model: 240000 steps (episode 7)\n",
      "Model: 241000 steps (episode 7)\n",
      "Model: 242000 steps (episode 7)\n",
      "Model: 243000 steps (episode 7)\n",
      "Model: 244000 steps (episode 7)\n",
      "Model: 245000 steps (episode 7)\n",
      "Model: 246000 steps (episode 7)\n",
      "Model: 247000 steps (episode 7)\n",
      "Model: 248000 steps (episode 7)\n",
      "Model: 249000 steps (episode 7)\n",
      "Model: 250000 steps (episode 7)\n",
      "Model: 251000 steps (episode 7)\n",
      "Model: 252000 steps (episode 7)\n",
      "Model: 253000 steps (episode 7)\n",
      "Model: 254000 steps (episode 7)\n",
      "Model: 255000 steps (episode 7)\n",
      "Model: 256000 steps (episode 7)\n",
      "Model: 257000 steps (episode 7)\n",
      "Model: 258000 steps (episode 7)\n",
      "Model: 259000 steps (episode 7)\n",
      "Model: 260000 steps (episode 7)\n",
      "Model: 261000 steps (episode 7)\n",
      "Model: 262000 steps (episode 7)\n",
      "Model: 263000 steps (episode 7)\n",
      "Model: 264000 steps (episode 7)\n",
      "Model: 265000 steps (episode 7)\n",
      "Model: 266000 steps (episode 7)\n",
      "Model: 267000 steps (episode 7)\n",
      "Model: 268000 steps (episode 7)\n",
      "Model: 269000 steps (episode 7)\n",
      "Model: 270000 steps (episode 7)\n",
      "Model: 271000 steps (episode 7)\n",
      "Model: 272000 steps (episode 7)\n",
      "Model: 273000 steps (episode 7)\n",
      "Model: 274000 steps (episode 7)\n",
      "Model: 275000 steps (episode 7)\n",
      "Model: 276000 steps (episode 7)\n",
      "Model: 277000 steps (episode 7)\n",
      "Model: 278000 steps (episode 7)\n",
      "Model: 279000 steps (episode 7)\n",
      "Model: 280000 steps (episode 7)\n",
      "Model: 280000 steps (episode 8)\n",
      "Model: 281000 steps (episode 8)\n",
      "Model: 282000 steps (episode 8)\n",
      "Model: 283000 steps (episode 8)\n",
      "Model: 284000 steps (episode 8)\n",
      "Model: 285000 steps (episode 8)\n",
      "Model: 286000 steps (episode 8)\n",
      "Model: 287000 steps (episode 8)\n",
      "Model: 288000 steps (episode 8)\n",
      "Model: 289000 steps (episode 8)\n",
      "Model: 290000 steps (episode 8)\n",
      "Model: 291000 steps (episode 8)\n",
      "Model: 292000 steps (episode 8)\n",
      "Model: 293000 steps (episode 8)\n",
      "Model: 294000 steps (episode 8)\n",
      "Model: 295000 steps (episode 8)\n",
      "Model: 296000 steps (episode 8)\n",
      "Model: 297000 steps (episode 8)\n",
      "Model: 298000 steps (episode 8)\n",
      "Model: 299000 steps (episode 8)\n",
      "Model: 300000 steps (episode 8)\n",
      "Model: 301000 steps (episode 8)\n",
      "Model: 302000 steps (episode 8)\n",
      "Model: 303000 steps (episode 8)\n",
      "Model: 304000 steps (episode 8)\n",
      "Model: 305000 steps (episode 8)\n",
      "Model: 306000 steps (episode 8)\n",
      "Model: 307000 steps (episode 8)\n",
      "Model: 308000 steps (episode 8)\n",
      "Model: 309000 steps (episode 8)\n",
      "Model: 310000 steps (episode 8)\n",
      "Model: 311000 steps (episode 8)\n",
      "Model: 312000 steps (episode 8)\n",
      "Model: 313000 steps (episode 8)\n",
      "Model: 314000 steps (episode 8)\n",
      "Model: 315000 steps (episode 8)\n",
      "Model: 316000 steps (episode 8)\n",
      "Model: 317000 steps (episode 8)\n",
      "Model: 318000 steps (episode 8)\n",
      "Model: 319000 steps (episode 8)\n",
      "Model: 320000 steps (episode 8)\n"
     ]
    }
   ],
   "source": [
    "timesteps = n_timesteps_past\n",
    "\n",
    "steps = []\n",
    "rew_disc_av = []\n",
    "\n",
    "# Test environmet\n",
    "env = RLA.EnvironmentPerceptron(x_arr=x_test,\n",
    "                                batch_size=batch_size,\n",
    "                                w_stud_0=w_stud_0, \n",
    "                                w_teach=w_teach, \n",
    "                                w_target=w_target, \n",
    "                                a_min=a_min,\n",
    "                                a_max=a_max,\n",
    "                                learning_rate=learning_rate, \n",
    "                                control_cost_weight=pref_control_cost_weight*control_cost_weight, \n",
    "                                activation=activation, \n",
    "                                randomise_initial_condition=randomise_initial_condition, \n",
    "                                shuffle_array=shuffle_array)\n",
    "\n",
    "# Testing\n",
    "for ep in range(n_episodes):\n",
    "    \n",
    "    for t in range(0, timesteps+1, save_freq):\n",
    "        \n",
    "        timesteps_past = ep*timesteps + t\n",
    "        steps_flag = False\n",
    "\n",
    "        if agent_model_name=='TD3':\n",
    "            path_to_model = path_agent + f'/rl_model_{timesteps_past}_steps.zip'\n",
    "            #path_to_buffer = path_repbuffer + f'rl_model_replay_buffer_{timesteps_past}_steps.pkl'\n",
    "            if os.path.exists(path_to_model):\n",
    "                model_iter = TD3.load(path_to_model, env)\n",
    "                #model_iter.load_replay_buffer(path_to_buffer)\n",
    "                print('Model: %d steps (episode %d)' % (timesteps_past, ep+1)) \n",
    "                steps_flag = True\n",
    "            else:\n",
    "                print('Model not found: %d steps (episode %d)' % (timesteps_past, ep+1)) \n",
    "\n",
    "        if steps_flag:\n",
    "            reward_list = []\n",
    "            obs, info = env.reset()\n",
    "            terminated = False\n",
    "            while not terminated:\n",
    "                action, _ = model_iter.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                if env.timestep>=n_timesteps - time_window:\n",
    "                    reward_list.append(reward)\n",
    "                    \n",
    "            rew_disc_av.append(np.mean(np.array(reward_list, dtype=object)))\n",
    "            steps.append(timesteps_past)\n",
    "        \n",
    "objective = -np.array(rew_disc_av, dtype=object)\n",
    "steps = np.array(steps, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ef679-54b9-4f7f-b9ac-0ddea8a05ad6",
   "metadata": {},
   "source": [
    "## Export training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7d79a43-5a17-4632-86b4-55cd60f9d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_description = '_agent#%s' % (agent_model_name)\n",
    "name = 'ObjectiveVSTrainingSteps'\n",
    "filename = name + '_@@@' + models_description + experiment_description\n",
    "data_to_export = objective\n",
    "np.save(rlmodels_path + filename, data_to_export)\n",
    "\n",
    "\n",
    "models_description = '_agent#%s' % (agent_model_name)\n",
    "name = 'TrainingSteps'\n",
    "filename = name + '_@@@' + models_description + experiment_description\n",
    "data_to_export = steps\n",
    "np.save(rlmodels_path + filename, data_to_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83423e-3a8d-4695-b612-361150096643",
   "metadata": {},
   "source": [
    "## Plot training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f53338c-f18f-4582-90f4-4bcbc37d24b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22018175761658085, 9999)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Greedy average reward\n",
    "if activation=='Linear':\n",
    "    pref_fut = 1.\n",
    "elif activation=='Erf':\n",
    "    pref_fut = 2.\n",
    "reward_greedy_list = []\n",
    "\n",
    "# Greedy experiment\n",
    "obs, info = env.reset()\n",
    "w_stud = env.w_stud\n",
    "x_batch = env.x_batch\n",
    "terminated = False\n",
    "while not terminated:    \n",
    "    action = GA.a_greedy_perceptron(w_stud=w_stud,\n",
    "                                    w_target=w_target,\n",
    "                                    w_teach=w_teach,\n",
    "                                    x_batch=x_batch,\n",
    "                                    x_buffer=x_buffer,\n",
    "                                    dim_input=dim_input,\n",
    "                                    eta=learning_rate,\n",
    "                                    weight_future=pref_fut*dim_input/learning_rate,\n",
    "                                    a_min=a_min,\n",
    "                                    a_max=a_max,\n",
    "                                    control_cost_weight=pref_control_cost_weight*control_cost_weight,\n",
    "                                    activation=activation)\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    w_stud = env.w_stud\n",
    "    x_batch = env.x_batch\n",
    "    if env.timestep>=n_timesteps - time_window:\n",
    "        reward_greedy_list.append(reward)\n",
    "    \n",
    "objective_greedy = -np.mean(np.array(reward_greedy_list, dtype=object))\n",
    "objective_greedy, env.timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f3f6fe6-6357-4b35-aab3-b5a685ea9e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274000, 0.22906867431559627)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find best model\n",
    "best_model_t = steps[np.argmin(objective)]\n",
    "best_model_obj = objective[np.argmin(objective)]\n",
    "best_model_t, best_model_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd0b25be-a4b0-4c7d-9d33-3b2e7472809a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 261x191.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AF.SetPlotParams(magnification=.9, ratio=float(2.2/3.), fontsize=11, lines_w=.7, ms=7)\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rc('text', usetex = True)\n",
    "mpl.rc('text.latex', preamble=r'\\usepackage{sfmath}')\n",
    "\n",
    "\n",
    "plt.axhline(objective_greedy, color='black', ls=':', lw=.8, label='greedy')\n",
    "plt.plot(steps+1, objective, color=orange)\n",
    "plt.plot(best_model_t, best_model_obj, color=light_blue, ls='', marker='.', ms=8)\n",
    "\n",
    "plt.ylabel('$g^{\\mathrm{RL}}_{\\infty}$')\n",
    "plt.xlabel('n. episodes')\n",
    "plt.xlim([-n_timesteps_past/4, n_episodes*n_timesteps_past])\n",
    "x_positions_ticks = np.linspace(0, n_episodes, n_episodes//2+1)*n_timesteps_past\n",
    "x_ticks_labels = [int(x) for x in np.linspace(0, n_episodes, n_episodes//2+1)]\n",
    "plt.xticks(x_positions_ticks, x_ticks_labels)\n",
    "plt.yticks([0.2, 0.4, 0.6, 0.8])\n",
    "plt.ylim([0.2, .8])\n",
    "plt.grid(False)\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aef6dc-04ce-4da1-a9bd-94fa2a0fa528",
   "metadata": {},
   "source": [
    "# Plot 1D policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8be3a901-80d0-49dd-8c73-8c378b0d2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dim_input==1 and batch_size==1:\n",
    "    \n",
    "    ########################\n",
    "    #     Choose model     #\n",
    "    ########################\n",
    "    \n",
    "    timesteps_past = best_model_t\n",
    "    if agent_model_name=='TD3':\n",
    "        path_to_model = path_agent + f'/rl_model_{timesteps_past}_steps.zip'\n",
    "        #path_to_buffer = path_repbuffer + f'rl_model_replay_buffer_{timesteps_past}_steps.pkl'\n",
    "        if os.path.exists(path_to_model):\n",
    "            model_iter = TD3.load(path_to_model, env)\n",
    "            #model_iter.load_replay_buffer(path_to_buffer)\n",
    "            print('%d (ep %d)' % (timesteps_past, ep+1)) \n",
    "            steps_flag = True\n",
    "        else:\n",
    "            print('  %d-model not found (ep %d)' % (timesteps_past, ep+1)) \n",
    "\n",
    "    AF.SetPlotParams(magnification=1.6, ratio=float(2.2/3.), fontsize=12, lines_w=1.3, ms=7)\n",
    "    mpl.rc('text', usetex = True)\n",
    "    mpl.rc('text.latex', preamble=r'\\usepackage{sfmath}')\n",
    "\n",
    "    \n",
    "    ########################\n",
    "    #        3D Plot       #\n",
    "    ########################\n",
    "    \n",
    "    #******  Evaluate policy functions  ******#\n",
    "    n_rho_values = 60\n",
    "    rho_grid = np.linspace(0., 1., n_rho_values)\n",
    "    n_x_values = 60\n",
    "    x_grid = np.linspace(-3, 3, n_x_values)\n",
    "    a_greedy_grid_2D = np.zeros((n_rho_values, n_x_values))\n",
    "    a_DeepRL_grid_2D = np.zeros((n_rho_values, n_x_values))\n",
    "    for j, rho in enumerate(rho_grid):\n",
    "        print('Line: %d/%d' % (j+1, n_rho_values))\n",
    "        w_stud_val = w_teach * (1-rho) + w_target * rho\n",
    "        a_greedy_grid = np.zeros_like(x_grid)\n",
    "        a_DeepRL_grid = np.zeros_like(x_grid)\n",
    "        for i, x in enumerate(x_grid):\n",
    "            x_batch = np.array([[x]], dtype=np.float32)\n",
    "            obs = np.concatenate((w_stud_val.reshape(1,-1), x_batch), axis=0, dtype=np.float32).flatten()\n",
    "            a_DeepRL_grid[i], _ = model_iter.predict(obs, deterministic=True)\n",
    "            a_greedy_grid[i] = GA.a_greedy_perceptron(w_stud=w_stud_val,\n",
    "                                                      w_target=w_target,\n",
    "                                                      w_teach=w_teach,\n",
    "                                                      x_batch=x_batch,\n",
    "                                                      x_buffer=x_buffer,\n",
    "                                                      dim_input=dim_input,\n",
    "                                                      eta=learning_rate,\n",
    "                                                      weight_future=pref_fut*dim_input/learning_rate,\n",
    "                                                      a_min=a_min,\n",
    "                                                      a_max=a_max,\n",
    "                                                      control_cost_weight=pref_control_cost_weight*control_cost_weight,\n",
    "                                                      activation=activation)\n",
    "        a_greedy_grid_2D[j] = a_greedy_grid\n",
    "        a_DeepRL_grid_2D[j] = a_DeepRL_grid\n",
    "        \n",
    "    \n",
    "    #******  Greedy policy  ******#\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    alpha = 0.8\n",
    "    \n",
    "    x = x_grid\n",
    "    for j, rho in enumerate(rho_grid):\n",
    "        y = 1-rho * np.ones_like(x)\n",
    "        z = a_greedy_grid_2D[j]\n",
    "        ax.plot(x, y, z, color=light_blue, alpha=alpha)\n",
    "        ax.set(xlabel='x', ylabel='$\\\\rho$', zlabel='a')\n",
    "        ax.set_zlabel('a')\n",
    "\n",
    "    # Remove fill\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    # Set color of edges\n",
    "    ax.xaxis.pane.set_edgecolor('grey')\n",
    "    ax.yaxis.pane.set_edgecolor('grey')\n",
    "    ax.zaxis.pane.set_edgecolor('grey')\n",
    "    ax.grid(False)\n",
    "    # Save figure\n",
    "    #filename = 'Policy1DGreedy#%s.pdf' % activation\n",
    "    #plt.savefig(path_figures + filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #******  RL agent policy  ******#\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    \n",
    "    for j, rho in enumerate(rho_grid):\n",
    "        y = 1-rho * np.ones_like(x)\n",
    "        z = a_DeepRL_grid_2D[j]\n",
    "        ax.plot(x, y, z, color=orange, alpha=alpha)\n",
    "        ax.set(xlabel='x', ylabel='$\\\\rho$', zlabel='a')\n",
    "        ax.set_zlabel('a')\n",
    "\n",
    "    # Remove fill\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    # Set color of edges\n",
    "    ax.xaxis.pane.set_edgecolor('grey')\n",
    "    ax.yaxis.pane.set_edgecolor('grey')\n",
    "    ax.zaxis.pane.set_edgecolor('grey')\n",
    "    ax.grid(False)\n",
    "    # Save figure\n",
    "    #filename = 'Policy1DRL#%s.pdf' % activation\n",
    "    #plt.savefig(path_figures + filename, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OptimalControlAttaks",
   "language": "python",
   "name": "optimalcontrolattaks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
