{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd7ac25-9549-40fb-95e7-704828689c02",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb9240-adde-4fb5-8cd4-dd5c1db39b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy, matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Stable baselines\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "import os\n",
    "local_path = '/Users/riccardo/Documents/GitHub/' #'path_to_progect_folder/'\n",
    "sys.path.append(local_path+'OptimalControlAttacks/SyntheticDataExperiments/')\n",
    "from Modules import AuxiliaryFunctions as AF\n",
    "from Modules import GreedyAttacks as GA\n",
    "from Modules import DeepRLAttacks as RLA\n",
    "from Parameters import ParametersAttacksComparison_2LayerNN as Par"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114b71d-1496-477a-8844-0699f5d46689",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559bc37-a2b8-4bcc-9f8b-bee3a6a7ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "orange = '#F5A962'\n",
    "light_blue = '#3C8DAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4618589-fcac-4a8e-9a86-5b6930378729",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa8b21-e7a8-4dce-9a29-7938df096043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = Par.model_name\n",
    "activation = Par.activation\n",
    "output_scaling = Par.output_scaling\n",
    "hiddenlayer_width = Par.hiddenlayer_width\n",
    "target_type = Par.target_type\n",
    "train_first_layer = Par.train_first_layer\n",
    "\n",
    "# Input data parameters\n",
    "dim_input = Par.dim_input\n",
    "batch_size = Par.batch_size\n",
    "mu_x = Par.mu_x\n",
    "sigma_x = Par.sigma_x\n",
    "n_runs_experiments = Par.n_runs_experiments\n",
    "\n",
    "# Dynamics parameters\n",
    "learning_rate = Par.learning_rate\n",
    "gamma = Par.gamma\n",
    "beta = Par.beta\n",
    "\n",
    "# N. samples\n",
    "n_timesteps = Par.n_timesteps\n",
    "n_timesteps_transient_th = Par.n_timesteps_transient_th\n",
    "n_timesteps_past = Par.n_timesteps_past\n",
    "n_samples_average = Par.n_samples_average\n",
    "n_samples_buffer = Par.n_samples_buffer\n",
    "n_samples_test = Par.n_samples_test\n",
    "time_window = Par.time_window\n",
    "\n",
    "# Control parameters\n",
    "a_min = Par.a_min\n",
    "a_max = Par.a_max\n",
    "n_a_gridpoints = Par.n_a_gridpoints\n",
    "n_runs_calibration = Par.n_runs_calibration\n",
    "control_cost_weight = Par.control_cost_weight\n",
    "greedy_weight_future = Par.greedy_weight_future\n",
    "opt_pref = Par.opt_pref\n",
    "fut_pref = Par.fut_pref\n",
    "\n",
    "# DeepRL Agent\n",
    "agent_model_name = Par.agent_model_name\n",
    "n_actions = Par.n_actions\n",
    "use_action_noise = Par.use_action_noise\n",
    "action_noise_mean = Par.action_noise_mean\n",
    "action_noise_std = Par.action_noise_std\n",
    "observation_type = Par.observation_type \n",
    "use_small_achitecture = Par.use_small_achitecture\n",
    "shuffle_array = Par.shuffle_array\n",
    "learning_rate_agent = Par.learning_rate_agent\n",
    "activation_fn = Par.activation_fn\n",
    "n_episodes = Par.n_episodes\n",
    "save_freq = Par.save_freq\n",
    "train_freq = Par.train_freq\n",
    "\n",
    "# Strings/paths\n",
    "experiment_description = Par.experiment_description\n",
    "path_agent = Par.path_agent\n",
    "export_path = Par.export_path\n",
    "rlmodels_path = Par.rlmodels_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cacbe8-0bec-4d2f-a891-2f2d7672e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dim_input:', dim_input)\n",
    "print('hiddenlayer_width:', hiddenlayer_width)\n",
    "print('output_scaling:', output_scaling)\n",
    "print('target_type:', target_type)\n",
    "print('batch_size:', batch_size)\n",
    "print('activation:', activation)\n",
    "print('a min:', a_min)\n",
    "print('a max:', a_max)\n",
    "print('observation_type:', observation_type)\n",
    "print('control_cost_weight:', control_cost_weight)\n",
    "print('N past timesteps:', n_timesteps_past)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c2160-13a5-4433-ba82-5c74f531ebbe",
   "metadata": {},
   "source": [
    "# Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdce99-5e89-4ca0-820a-f8341fd34beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "# Teacher\n",
    "W_teach = np.random.normal(0, 1, (hiddenlayer_width, dim_input))\n",
    "W_teach = W_teach / (np.sum(W_teach**2, axis=1).reshape(-1,1).repeat(dim_input, axis=1)/dim_input)**0.5\n",
    "v_teach = np.random.normal(0, 1, hiddenlayer_width)\n",
    "v_teach = v_teach / ((np.sum(v_teach**2)/hiddenlayer_width))**0.5\n",
    "# Target\n",
    "if target_type=='FlippedTeacher':\n",
    "    W_target = W_teach.copy()\n",
    "    v_target = -v_teach.copy()\n",
    "elif target_type=='Random':\n",
    "    W_target = np.random.normal(0, 1, (hiddenlayer_width, dim_input))\n",
    "    W_target = W_target / (np.sum(W_target**2, axis=1).reshape(-1,1).repeat(dim_input, axis=1)/dim_input)**0.5\n",
    "    v_target = np.random.normal(0, 1, hiddenlayer_width)\n",
    "    v_target = v_target / ((np.sum(v_target**2)/hiddenlayer_width))**0.5\n",
    "# Student (initial condition)\n",
    "W_stud_0 = W_teach.copy()\n",
    "v_stud_0 = v_teach.copy()\n",
    "            \n",
    "# Arrays (assuming batch_size as specified above)\n",
    "x_incoming_arr = np.random.normal(mu_x, sigma_x, (n_runs_experiments, batch_size*n_timesteps, dim_input))\n",
    "x_past = np.random.normal(mu_x, sigma_x, (batch_size*n_timesteps_past, dim_input))\n",
    "x_buffer = np.random.normal(mu_x, sigma_x, (batch_size*n_samples_buffer, dim_input))\n",
    "x_test = np.random.normal(mu_x, sigma_x, (batch_size*n_samples_test, dim_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a74ee2-f560-4fc6-8ac6-23a23e85c8a3",
   "metadata": {},
   "source": [
    "# Weight action cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bb0da-8267-402b-afd6-d1a73cbff836",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_t_test = GA.NN2L(x_test, W_teach, v_teach, activation=activation, output_scaling=output_scaling)\n",
    "label_o_test = GA.NN2L(x_test, W_target, v_target, activation=activation, output_scaling=output_scaling)\n",
    "error_target_teach = np.mean((label_o_test-label_t_test)**2)\n",
    "d_target_teach = 0.5 * error_target_teach\n",
    "pref_control_cost_weight = (2*d_target_teach)\n",
    "print('Pre-factor control cost: %.2f' % pref_control_cost_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa3246-8abc-4ea4-9ba9-faedc9974514",
   "metadata": {},
   "source": [
    "## Check environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387dd4a-209a-47a4-8c76-7d2e760cf46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shuffle past stream:', shuffle_array)\n",
    "env = RLA.EnvironmentNN2L(observation_type=observation_type,\n",
    "                          x_arr=x_past,\n",
    "                          batch_size=batch_size,\n",
    "                          W_stud_0=W_stud_0, \n",
    "                          v_stud_0=v_stud_0,\n",
    "                          W_teach=W_teach, \n",
    "                          v_teach=v_teach, \n",
    "                          W_target=W_target, \n",
    "                          v_target=v_target, \n",
    "                          a_min=a_min,\n",
    "                          a_max=a_max,\n",
    "                          learning_rate=learning_rate, \n",
    "                          control_cost_weight=pref_control_cost_weight*control_cost_weight, \n",
    "                          activation=activation,\n",
    "                          shuffle_array=shuffle_array, \n",
    "                          outputscaling=output_scaling, \n",
    "                          train_first_layer=train_first_layer)\n",
    "\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9244746b-a523-4ed5-9014-f200904aded2",
   "metadata": {},
   "source": [
    "# RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01092f40-37ae-4e4a-98a7-202f1b7339d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small architecture\n",
    "if use_small_achitecture:\n",
    "    print('Using custom small architecture for policy and value!')\n",
    "    custom_net_kwargs = dict(activation_fn=activation_fn, net_arch=dict(pi=[10, 10], qf=[10, 10]))\n",
    "else:\n",
    "    custom_net_kwargs = None\n",
    "         \n",
    "# Create model\n",
    "if agent_model_name=='TD3':\n",
    "    \n",
    "        if use_action_noise:\n",
    "            action_noise = NormalActionNoise(mean=action_noise_mean, sigma=action_noise_std)\n",
    "        else:\n",
    "            action_noise = None  \n",
    "            \n",
    "        model = TD3(\"MlpPolicy\", \n",
    "                    env,\n",
    "                    action_noise=action_noise,\n",
    "                    verbose=0, \n",
    "                    gamma=gamma,\n",
    "                    learning_rate=learning_rate_agent, \n",
    "                    policy_kwargs=custom_net_kwargs,\n",
    "                    train_freq=train_freq, \n",
    "                    gradient_steps=train_freq)\n",
    "        print('New %s agent created' % agent_model_name)\n",
    "else:\n",
    "    raise ValueError('Agent type other than TD3 not available/tested')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2dd4e-0be8-4b4c-84ed-0a11014ea226",
   "metadata": {},
   "source": [
    "# Training agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c65c75-1342-452c-9595-2973254964a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('timesteps for each episode:', n_timesteps_past)\n",
    "\n",
    "# Save a checkpoint \n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=save_freq,\n",
    "  save_path=path_agent,\n",
    "  name_prefix=\"rl_model\",\n",
    "  save_replay_buffer=False,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    print('Episode: %d/%d'%(ep+1, n_episodes))\n",
    "    model.learn(total_timesteps=n_timesteps_past, \n",
    "                progress_bar=False, \n",
    "                reset_num_timesteps=False,\n",
    "                tb_log_name='DeepRL', \n",
    "                log_interval=1, \n",
    "                callback=checkpoint_callback)\n",
    "        \n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf258824-0032-4df3-a232-5011374a5834",
   "metadata": {},
   "source": [
    "## Compute training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8b78d-878c-4243-b0ab-17df62b9e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = n_timesteps_past\n",
    "\n",
    "steps = []\n",
    "rew_disc_av = []\n",
    "\n",
    "# Test environment\n",
    "env = RLA.EnvironmentNN2L(observation_type=observation_type,\n",
    "                          x_arr=x_test,\n",
    "                          batch_size=batch_size,\n",
    "                          W_stud_0=W_stud_0, \n",
    "                          v_stud_0=v_stud_0,\n",
    "                          W_teach=W_teach, \n",
    "                          v_teach=v_teach, \n",
    "                          W_target=W_target, \n",
    "                          v_target=v_target, \n",
    "                          a_min=a_min,\n",
    "                          a_max=a_max,\n",
    "                          learning_rate=learning_rate, \n",
    "                          control_cost_weight=pref_control_cost_weight*control_cost_weight, \n",
    "                          activation=activation,\n",
    "                          shuffle_array=shuffle_array, \n",
    "                          outputscaling=output_scaling, \n",
    "                          train_first_layer=train_first_layer)\n",
    "# Testing\n",
    "for ep in range(n_episodes):\n",
    "    \n",
    "    for t in range(0, timesteps+1, save_freq):\n",
    "        \n",
    "        timesteps_past = ep*timesteps + t\n",
    "        steps_flag = False\n",
    "\n",
    "        if agent_model_name=='TD3':\n",
    "            path_to_model = path_agent + f'/rl_model_{timesteps_past}_steps.zip'\n",
    "            #path_to_buffer = path_repbuffer + f'rl_model_replay_buffer_{timesteps_past}_steps.pkl'\n",
    "            if os.path.exists(path_to_model):\n",
    "                model_iter = TD3.load(path_to_model, env)\n",
    "                #model_iter.load_replay_buffer(path_to_buffer)\n",
    "                print('Model: %d steps (episode %d)' % (timesteps_past, ep+1)) \n",
    "                steps_flag = True\n",
    "            else:\n",
    "                print('Model not found: %d steps (episode %d)' % (timesteps_past, ep+1)) \n",
    "\n",
    "        if steps_flag:\n",
    "            reward_list = []\n",
    "            obs, info = env.reset()\n",
    "            terminated = False\n",
    "            while not terminated:\n",
    "                action, _ = model_iter.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                if env.timestep>=n_timesteps - time_window:\n",
    "                    reward_list.append(reward)\n",
    "                    \n",
    "            rew_disc_av.append(np.mean(np.array(reward_list, dtype=object)))\n",
    "            steps.append(timesteps_past)\n",
    "        \n",
    "objective = -np.array(rew_disc_av, dtype=object)\n",
    "steps = np.array(steps, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ef679-54b9-4f7f-b9ac-0ddea8a05ad6",
   "metadata": {},
   "source": [
    "## Export training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d79a43-5a17-4632-86b4-55cd60f9d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_description = '_agent#%s' % (agent_model_name)\n",
    "name = 'ObjectiveVSTrainingSteps'\n",
    "filename = name + '_@@@' + models_description + experiment_description\n",
    "data_to_export = objective\n",
    "np.save(rlmodels_path + filename, data_to_export)\n",
    "\n",
    "\n",
    "models_description = '_agent#%s' % (agent_model_name)\n",
    "name = 'TrainingSteps'\n",
    "filename = name + '_@@@' + models_description + experiment_description\n",
    "data_to_export = steps\n",
    "np.save(rlmodels_path + filename, data_to_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83423e-3a8d-4695-b612-361150096643",
   "metadata": {},
   "source": [
    "## Plot training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f53338c-f18f-4582-90f4-4bcbc37d24b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy average reward\n",
    "if activation=='Linear':\n",
    "    pref_fut = 1.\n",
    "elif activation=='Erf':\n",
    "    pref_fut = 1.6 # 2\n",
    "reward_greedy_list = []\n",
    "\n",
    "# Greedy experiment\n",
    "obs, info = env.reset()\n",
    "W_stud = env.W_stud\n",
    "v_stud = env.v_stud\n",
    "x_batch = env.x_batch\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = GA.a_greedy_NN2L(W_stud=W_stud,\n",
    "                              v_stud=v_stud,\n",
    "                              W_target=W_target,\n",
    "                              v_target=v_target,\n",
    "                              W_teach=W_teach,\n",
    "                              v_teach=v_teach,\n",
    "                              x_batch=x_batch,\n",
    "                              x_buffer=x_buffer,\n",
    "                              dim_input=dim_input,\n",
    "                              eta=learning_rate,\n",
    "                              weight_future=pref_fut*dim_input/learning_rate,\n",
    "                              a_min=a_min,\n",
    "                              a_max=a_max,\n",
    "                              n_gridpoints=n_a_gridpoints,\n",
    "                              control_cost_weight=pref_control_cost_weight*control_cost_weight,\n",
    "                              activation=activation,\n",
    "                              train_first_layer=train_first_layer,\n",
    "                              output_scaling=output_scaling)\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    W_stud, v_stud = env.W_stud, env.v_stud\n",
    "    x_batch = env.x_batch\n",
    "    if env.timestep>=n_timesteps - time_window:\n",
    "        reward_greedy_list.append(reward)\n",
    "    if env.timestep%100==0:\n",
    "        print(env.timestep)\n",
    "    \n",
    "objective_greedy = -np.mean(np.array(reward_greedy_list, dtype=object))\n",
    "objective_greedy, env.timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f6fe6-6357-4b35-aab3-b5a685ea9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_model_t = steps[np.argmin(objective)]\n",
    "best_model_obj = objective[np.argmin(objective)]\n",
    "best_model_t, best_model_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b25be-a4b0-4c7d-9d33-3b2e7472809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AF.SetPlotParams(magnification=.9, ratio=float(2.2/3.), fontsize=11, lines_w=.7, ms=7)\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rc('text', usetex = True)\n",
    "mpl.rc('text.latex', preamble=r'\\usepackage{sfmath}')\n",
    "\n",
    "\n",
    "plt.axhline(objective_greedy, color='black', ls=':', lw=.8, label='greedy')\n",
    "plt.plot(steps+1, objective, color=orange)\n",
    "plt.plot(best_model_t, best_model_obj, color=light_blue, ls='', marker='.', ms=8)\n",
    "\n",
    "plt.ylabel('$g^{\\mathrm{RL}}_{\\infty}$')\n",
    "plt.xlabel('n. episodes')\n",
    "plt.xlim([-n_timesteps_past/4, n_episodes*n_timesteps_past])\n",
    "x_positions_ticks = np.linspace(0, n_episodes, n_episodes//2+1)*n_timesteps_past\n",
    "x_ticks_labels = [int(x) for x in np.linspace(0, n_episodes, n_episodes//2+1)]\n",
    "plt.xticks(x_positions_ticks, x_ticks_labels)\n",
    "plt.yticks([0.5, 1., 1.5, 2.])\n",
    "plt.ylim([0.5, 2.])\n",
    "plt.grid(False)\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OptimalControlAttaks",
   "language": "python",
   "name": "optimalcontrolattaks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
